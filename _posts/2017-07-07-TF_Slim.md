---
layout:   single
title: "Tensorflow-Slim"
date: 2017-07-04 22:58:00
categories: Tensorflow
tags: Tensorflow
---

Tensorflow是当前最火的机器学习库之一，它的优点和缺点都十分明显，就是太过灵活。我自己写TF程序的时候都需要经过精细的设计和包装，才能比较顺手的使用TF，把更多精力放在模型设计上。
然而，最近发现TensorFlow的子库Slim，这是一个轻量化的库，用于定义、训练、测评复杂的模型，感觉自己之前的代码都白写了（当然很好得锻炼了自己的代码能力，看了Slim代码后有更深的感触），早知道有这么方便的库，就不用自己花那么多时间了。
[Tensorflow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)
{% include toc %}

## 为什么选择TF-Slim
因为简单方便。

* 让调用者写出更加简洁的代码。这是由[argument scoping](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/arg_scope.py)和大量的高级包装的[layers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py)和[variables](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py)实现的
* 写模型更加简单，比如提供经常使用的[regularizers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py)
* 提供经常使用的计算机视觉模型(e.g. VGG, AlexNet)，就算你是计算机视觉小白也能把它们当黑盒子来使用，只要你想，还能十分简单地加几层来改模型。[Models](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets/)
* 使得更简单地区拓展复杂的模型，比如使用已经训练好的模型参数。

## Defining Models
### Variables

在原本的TensorFlow中创建`Variables`，需要预定义一个值或者提供初始化机制。此外，如果你想一个变量创建于特定的设备上，比如GPU，你需要具体地写出来。所以我一般都会包装起来再调用，而Slim也做了这个工作。现在，用Slim创建一个`weights`变量，使用截断的正态分布来初始化，并且使用`l2_loss`来调整参数，把变量放在`CPU`上：
```python
weights = slim.variable('weights',
                             shape=[10, 10, 3 , 3],
                             initializer=tf.truncated_normal_initializer(stddev=0.1),
                             regularizer=slim.l2_regularizer(0.05),
                             device='/CPU:0')
```

值得注意的是，在原本的TensorFlow中，有两种变量：`regular variables`和`local(transient) variables`。大多数变量都是regular variables，一旦被创建，他们可以被`saver`存到磁盘上。而local variables是只存在于Session期间、不被存储在磁盘上。

TF-Slim进一步把变量区分开来，定义了`model variables`，这是用于表示模型的参数。model variables用于训练或者fine-tune，并且从checkpoint载入来前馈。   
其他非model variable会在学习或者测评阶段使用，但是在前馈的时候不会被使用。
例如，`globale_step`就是一个在训练和测评时使用的变量，但是并不是模型的一部分。

`moidel variables`和`regular variables`都是以很简单地通过TF-Slim来创建：
```python
# Model Variables
weights = slim.model_variable('weights',
                              shape=[10, 10, 3 , 3],
                              initializer=tf.truncated_normal_initializer(stddev=0.1),
                              regularizer=slim.l2_regularizer(0.05),
                              device='/CPU:0')
model_variables = slim.get_model_variables()

# Regular variables
my_var = slim.variable('my_var',
                       shape=[20, 1],
                       initializer=tf.zeros_initializer())
regular_variables_and_model_variables = slim.get_variables()
```

它到底是怎么运作的？当你使用TF-Slim的层或者直接通过slim.model_variable函数来创建`model variables`时，TF-Slim就回把变量加到`tf.GraphKeys.MODEL_VARIABLES`集合中。当然你可以自定义变量并加到model variables这个集合中
```python
my_model_variable = CreateViaCustomCode()

# Letting TF-Slim know about the additional variable.
slim.add_model_variable(my_model_variable)
```

### Layers
TensorFlow的操作还是相当广泛的，然而搭建神经网络的小伙伴往往想到的是更高级的概念，比如模型的“层”，“损失值”，“衡量标准”，“网络架构“等等。对于一个层来说，除了对应的操作，往往还包含变量。比如卷积层就有以下几个操作：

1. 创建权重和偏置
2. 在输入上进行卷积
3. 把偏置加在卷积结果上
4. 用激活函数作用于以上结果

如果你使用普通的TF代码，写一个卷积层你需要这么多代码：
```python
input = ...
with tf.name_scope('conv1_1') as scope:
  kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,
                                           stddev=1e-1), name='weights')
  conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')
  biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),
                       trainable=True, name='biases')
  bias = tf.nn.bias_add(conv, biases)
  conv1 = tf.nn.relu(bias, name=scope)
```
为了简化以上操作（也就是我自己之前写的包装，当然还可以加入更多功能），用TF-Slime写只需要一行
```python
input = ...
net = slim.conv2d(input, 128, [3, 3], scope='conv1_1')
```
更可怕的，TF-Slim提供两个元操作`repeat`和`stack`来让你写出更加简洁的代码来重复进行相同的操作。比如VGG网络中，有这种重复的代码
```python
net = ...
net = slim.conv2d(net, 256, [3, 3], scope='conv3_1')
net = slim.conv2d(net, 256, [3, 3], scope='conv3_2')
net = slim.conv2d(net, 256, [3, 3], scope='conv3_3')
net = slim.max_pool2d(net, [2, 2], scope='pool2')
```
有一种方法是可以这么写，使用`for`循环
```python
net = ...
for i in range(3):
  net = slim.conv2d(net, 256, [3, 3], scope='conv3_' % (i+1))
net = slim.max_pool2d(net, [2, 2], scope='pool2')
```
而用TF-Slim的`repeat`操作符
```python
net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
net = slim.max_pool2d(net, [2, 2], scope='pool2')
```   
`slim.repeat`不仅重复使用相同的argument，而且它能够自己展开scope，比如上面的，就会被展开命名为'conv3/conv3_1', 'conv3/conv3_2', 'conv3/conv3_3'。

`slim.stack`操作符则允许调用者重复调用相同的操作而使用不同的argument来创建层。
```python
# Verbose way:
x = slim.fully_connected(x, 32, scope='fc/fc_1')
x = slim.fully_connected(x, 64, scope='fc/fc_2')
x = slim.fully_connected(x, 128, scope='fc/fc_3')

# Equivalent, TF-Slim way using slim.stack:
slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')
```
```python
# Verbose way:
x = slim.conv2d(x, 32, [3, 3], scope='core/core_1')
x = slim.conv2d(x, 32, [1, 1], scope='core/core_2')
x = slim.conv2d(x, 64, [3, 3], scope='core/core_3')
x = slim.conv2d(x, 64, [1, 1], scope='core/core_4')

# Using stack:
slim.stack(x, slim.conv2d, [(32, [3, 3]), (32, [1, 1]), (64, [3, 3]), (64, [1, 1])], scope='core')
```

### Scopes
处理TensorFlow本身的范围机制`name_scope`，`variable_scope`，TF-Slim还增加了`arg_scope`范围机制。这个新的范围允许使用者来明确表示一个或多个操作和一系列的arguments（会被传到对应的操作中）。还是用例子来解释吧：
```python
net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME',
                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv1')
net = slim.conv2d(net, 128, [11, 11], padding='VALID',
                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv2')
net = slim.conv2d(net, 256, [11, 11], padding='SAME',
                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv3')
```    
显然，上面三个卷积层使用很多相同的参数。比如两个使用相同的padding，三个使用相同的权重初始方法和权重调整方法。这样的代码十分冗余，也很难读，包含一堆重复的值。对于经常编程的人来说，有一个习惯可以解决以上问题，就是使用变量来表达默认值：
```python
padding = 'SAME'
initializer = tf.truncated_normal_initializer(stddev=0.01)
regularizer = slim.l2_regularizer(0.0005)
net = slim.conv2d(inputs, 64, [11, 11], 4,
                  padding=padding,
                  weights_initializer=initializer,
                  weights_regularizer=regularizer,
                  scope='conv1')
net = slim.conv2d(net, 128, [11, 11],
                  padding='VALID',
                  weights_initializer=initializer,
                  weights_regularizer=regularizer,
                  scope='conv2')
net = slim.conv2d(net, 256, [11, 11],
                  padding=padding,
                  weights_initializer=initializer,
                  weights_regularizer=regularizer,
                  scope='conv3')
```
但是有更加简便的方法，就是使用`arg_scope`。
```python
  with slim.arg_scope([slim.conv2d], padding='SAME',
                      weights_initializer=tf.truncated_normal_initializer(stddev=0.01)
                      weights_regularizer=slim.l2_regularizer(0.0005)):
    net = slim.conv2d(inputs, 64, [11, 11], scope='conv1')
    net = slim.conv2d(net, 128, [11, 11], padding='VALID', scope='conv2')
    net = slim.conv2d(net, 256, [11, 11], scope='conv3')

```
第二个从重写了Padding参数。还可以使用嵌入式`arg_scopes`
```python
with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                      weights_regularizer=slim.l2_regularizer(0.0005)):
  with slim.arg_scope([slim.conv2d], stride=1, padding='SAME'):
    net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID', scope='conv1')
    net = slim.conv2d(net, 256, [5, 5],
                      weights_initializer=tf.truncated_normal_initializer(stddev=0.03),
                      scope='conv2')
    net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc')
```
举个栗子
写个VGGnet
```python
def vgg16(inputs):
  with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),
                      weights_regularizer=slim.l2_regularizer(0.0005)):
    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
    net = slim.max_pool2d(net, [2, 2], scope='pool1')
    net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
    net = slim.max_pool2d(net, [2, 2], scope='pool2')
    net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
    net = slim.max_pool2d(net, [2, 2], scope='pool3')
    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')
    net = slim.max_pool2d(net, [2, 2], scope='pool4')
    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
    net = slim.max_pool2d(net, [2, 2], scope='pool5')
    net = slim.fully_connected(net, 4096, scope='fc6')
    net = slim.dropout(net, 0.5, scope='dropout6')
    net = slim.fully_connected(net, 4096, scope='fc7')
    net = slim.dropout(net, 0.5, scope='dropout7')
    net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc8')
  return net
```
就是这么简单。

## Training Models
训练一个TF模型需要一个模型，一个损失函数，一个梯度计算方法和一个训练routine，通过多次计算梯度来更新模型的权重。TF-Slim提供损失函数和一系列帮助函数来跑训练和评测routines。

### Losses
损失函数就是一个量我们喜欢最小化的。对于分类问题，很典型的就是使用交叉熵来衡量真实分布于预测分布之间的差别。而对于回归问题，常使用均方差来表示预测是正确的之前的差别。

某些特定的模型，比如多任务模型，需要同时使用多个损失函数。换句话说，损失函数最终是由多种其他损失函数之和来最小化的。

单任务
```python
import tensorflow as tf
vgg = tf.contrib.slim.nets.vgg

# Load the images and labels.
images, labels = ...

# Create the model.
predictions, _ = vgg.vgg_16(images)

# Define the loss functions and get the total loss.
loss = slim.losses.softmax_cross_entropy(predictions, labels)
```
多任务
```python
# Load the images and labels.
images, scene_labels, depth_labels = ...

# Create the model.
scene_predictions, depth_predictions = CreateMultiTaskModel(images)

# Define the loss functions and get the total loss.
classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)
sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)

# The following two lines have the same effect:
total_loss = classification_loss + sum_of_squares_loss
total_loss = slim.losses.get_total_loss(add_regularization_losses=False)
```
在以上例子中，我们调用`slim.losses.softmax_cross_entropy`和`slim.losses.sum_of_squares`。我们还可以通过把它们加起来或者调用`slim.losses.get_total_loss()`来获得总损失值。什么原理？当你通过TF-Slim来创建损失函数时，TF-Slim会把loss加到一个特定的TF损失函数集合中。

如果你有自定义的损失函数，可以如下调用函数：
```python
# Load the images and labels.
images, scene_labels, depth_labels, pose_labels = ...

# Create the model.
scene_predictions, depth_predictions, pose_predictions = CreateMultiTaskModel(images)

# Define the loss functions and get the total loss.
classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)
sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)
pose_loss = MyCustomLossFunction(pose_predictions, pose_labels)
slim.losses.add_loss(pose_loss) # Letting TF-Slim know about the additional loss.

# The following two ways to compute the total loss are equivalent:
regularization_loss = tf.add_n(slim.losses.get_regularization_losses())
total_loss1 = classification_loss + sum_of_squares_loss + pose_loss + regularization_loss

# (Regularization Loss is included in the total loss by default).
total_loss2 = slim.losses.get_total_loss()
```

### Training Loop
TF-Slim提供一系列简单但强大的工具来训练模型，函数在这[learning.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py)。这里包括训练函数——不断计算损失值和梯度并保存模型到磁盘上，还有几个方便的函数来处理梯度。例如，一旦我们定义好模型，损失函数和优化方法，我们就可以调用`slim.learning.create_train_op`和`slim.learning.train`来进行优化。
```python
g = tf.Graph()

# Create the model and specify the losses...
...

total_loss = slim.losses.get_total_loss()
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

# create_train_op ensures that each time we ask for the loss, the update_ops
# are run and the gradients being computed are applied too.
train_op = slim.learning.create_train_op(total_loss, optimizer)
logdir = ... # Where checkpoints are stored.

slim.learning.train(
    train_op,
    logdir,
    number_of_steps=1000,
    save_summaries_secs=300,
    save_interval_secs=600):
```

训练VGG16模型
```python
import tensorflow as tf

slim = tf.contrib.slim
vgg = tf.contrib.slim.nets.vgg

...

train_log_dir = ...
if not tf.gfile.Exists(train_log_dir):
  tf.gfile.MakeDirs(train_log_dir)

with tf.Graph().as_default():
  # Set up the data loading:
  images, labels = ...

  # Define the model:
  predictions = vgg.vgg16(images, is_training=True)

  # Specify the loss function:
  slim.losses.softmax_cross_entropy(predictions, labels)

  total_loss = slim.losses.get_total_loss()
  tf.summary.scalar('losses/total_loss', total_loss)

  # Specify the optimization scheme:
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)

  # create_train_op that ensures that when we evaluate it to get the loss,
  # the update_ops are done and the gradient updates are computed.
  train_tensor = slim.learning.create_train_op(total_loss, optimizer)

  # Actually runs training.
  slim.learning.train(train_tensor, train_log_dir)
```

## Fine-Tuning Exsting Models
### Brief Recap on Restoring Variables from a Checkpoint
训练好一个模型后，可以使用`tf.train.Saver()`来从`checkpoint`中取回`Variables`
```python
# Create some variables.
v1 = tf.Variable(..., name="v1")
v2 = tf.Variable(..., name="v2")
...
# Add ops to restore all the variables.
restorer = tf.train.Saver()

# Add ops to restore some variables.
restorer = tf.train.Saver([v1, v2])

# Later, launch the model, use the saver to restore variables from disk, and
# do some work with the model.
with tf.Session() as sess:
  # Restore variables from disk.
  restorer.restore(sess, "/tmp/model.ckpt")
  print("Model restored.")
  # Do some work with the model
  ...
```
### Partailly Restoring Models
```python
# Create some variables.
v1 = slim.variable(name="v1", ...)
v2 = slim.variable(name="nested/v2", ...)
...

# Get list of variables to restore (which contains only 'v2'). These are all
# equivalent methods:
variables_to_restore = slim.get_variables_by_name("v2")
# or
variables_to_restore = slim.get_variables_by_suffix("2")
# or
variables_to_restore = slim.get_variables(scope="nested")
# or
variables_to_restore = slim.get_variables_to_restore(include=["nested"])
# or
variables_to_restore = slim.get_variables_to_restore(exclude=["v1"])

# Create the saver which will be used to restore the variables.
restorer = tf.train.Saver(variables_to_restore)

with tf.Session() as sess:
  # Restore variables from disk.
  restorer.restore(sess, "/tmp/model.ckpt")
  print("Model restored.")
  # Do some work with the model
  ...
```

### Restoring models with different variable names
当从checkpoint中取出变量时，`Saver`会定位根据名字在checkpoint中定位变量并且把它们映射到当前图中的变量。这过程中，我们创建一个saver，并传递一个变量的列表给它。这样，被定位的变量的名字隐式地从`var.op.name`里得到。

当checkpoint中的变量名和graph中的变量名相匹配的情况下，这样没什么问题。但是有时，我们希望从一个checkpoint中取出一个模型，但是在当前graph中有不同的变量名。这样，我们必须给`Saver`提供一个dictionary来把checkpoint的变量名映射到graph的变量名上。可以用一下简单的函数来对变量名进行变换
```python
# Assuming than 'conv1/weights' should be restored from 'vgg16/conv1/weights'
def name_in_checkpoint(var):
  return 'vgg16/' + var.op.name

# Assuming than 'conv1/weights' and 'conv1/bias' should be restored from 'conv1/params1' and 'conv1/params2'
def name_in_checkpoint(var):
  if "weights" in var.op.name:
    return var.op.name.replace("weights", "params1")
  if "bias" in var.op.name:
    return var.op.name.replace("bias", "params2")

variables_to_restore = slim.get_model_variables()
variables_to_restore = {name_in_checkpoint(var):var for var in variables_to_restore}
restorer = tf.train.Saver(variables_to_restore)

with tf.Session() as sess:
  # Restore variables from disk.
  restorer.restore(sess, "/tmp/model.ckpt")
```

### Fine-Tuning a Model on a Different Task
```python
# Load the Pascal VOC data
image, label = MyPascalVocDataLoader(...)
images, labels = tf.train.batch([image, label], batch_size=32)

# Create the model
predictions = vgg.vgg_16(images)

train_op = slim.learning.create_train_op(...)

# Specify where the Model, trained on ImageNet, was saved.
model_path = '/path/to/pre_trained_on_imagenet.checkpoint'

# Specify where the new model will live:
log_dir = '/path/to/my_pascal_model_dir/'

# Restore only the convolutional layers:
variables_to_restore = slim.get_variables_to_restore(exclude=['fc6', 'fc7', 'fc8'])
init_fn = assign_from_checkpoint_fn(model_path, variables_to_restore)

# Start training.
slim.learning.train(train_op, log_dir, init_fn=init_fn)
```

## Evaluating Models
[通向原文](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)
