---
layout:   single
title: "Tensorflow-Slim"
date: 2017-07-04 22:58:00
categories: Tensorflow
tags: Tensorflow
---

Tensorflow是当前最火的机器学习库之一，它的优点和缺点都十分明显，就是太过灵活。我自己写TF程序的时候都需要经过精细的设计和包装，才能比较顺手的使用TF，把更多精力放在模型设计上。
然而，最近发现TensorFlow的子库Slim，这是一个轻量化的库，用于定义、训练、测评复杂的模型，感觉自己之前的代码都白写了（当然很好得锻炼了自己的代码能力，看了Slim代码后有更深的感触），早知道有这么方便的库，就不用自己花那么多时间了。
[Tensorflow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)
{% include toc %}

## 为什么选择TF-Slim
因为简单方便。

* 让调用者写出更加简洁的代码。这是由[argument scoping](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/arg_scope.py)和大量的高级包装的[layers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py)和[variables](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py)实现的
* 写模型更加简单，比如提供经常使用的[regularizers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py)
* 提供经常使用的计算机视觉模型(e.g. VGG, AlexNet)，就算你是计算机视觉小白也能把它们当黑盒子来使用，只要你想，还能十分简单地加几层来改模型。[我是模型](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets/)
* 使得更简单地区拓展复杂的模型，比如使用已经训练好的模型参数。

## Defining Models
### Variables

在原本的TensorFlow中创建`Variables`，需要预定义一个值或者提供初始化机制。此外，如果你想一个变量创建于特定的设备上，比如GPU，你需要具体地写出来。所以我一般都会包装起来再调用，而Slim也做了这个工作。现在，用Slim创建一个`weights`变量，使用截断的正态分布来初始化，并且使用`l2_loss`来调整参数，把变量放在`CPU`上：
```python
weights = slim.variable('weights',
                             shape=[10, 10, 3 , 3],
                             initializer=tf.truncated_normal_initializer(stddev=0.1),
                             regularizer=slim.l2_regularizer(0.05),
                             device='/CPU:0')
```

值得注意的是，在原本的TensorFlow中，有两种变量：`regular variables`和`local(transient) variables`。大多数变量都是regular variables，一旦被创建，他们可以被`saver`存到磁盘上。而local variables是只存在于Session期间、不被存储在磁盘上。

TF-Slim进一步把变量区分开来，定义了`model variables`，这是用于表示模型的参数。model variables用于训练或者fine-tune，并且从checkpoint载入来前馈。   
其他非model variable会在学习或者测评阶段使用，但是在前馈的时候不会被使用。
例如，`globale_step`就是一个在训练和测评时使用的变量，但是并不是模型的一部分。

`moidel variables`和`regular variables`都是以很简单地通过TF-Slim来创建：
```python
# Model Variables
weights = slim.model_variable('weights',
                              shape=[10, 10, 3 , 3],
                              initializer=tf.truncated_normal_initializer(stddev=0.1),
                              regularizer=slim.l2_regularizer(0.05),
                              device='/CPU:0')
model_variables = slim.get_model_variables()

# Regular variables
my_var = slim.variable('my_var',
                       shape=[20, 1],
                       initializer=tf.zeros_initializer())
regular_variables_and_model_variables = slim.get_variables()
```

它到底是怎么运作的？当你使用TF-Slim的层或者直接通过slim.model_variable函数来创建`model variables`时，TF-Slim就回把变量加到`tf.GraphKeys.MODEL_VARIABLES`集合中。当然你可以自定义变量并加到model variables这个集合中
```python
my_model_variable = CreateViaCustomCode()

# Letting TF-Slim know about the additional variable.
slim.add_model_variable(my_model_variable)
```

### Layers
TensorFlow的操作还是相当广泛的，然而搭建神经网络的小伙伴往往想到的是更高级的概念，比如模型的“层”，“损失值”，“衡量标准”，“网络架构“等等。对于一个层来说，除了对应的操作，往往还包含变量。比如卷积层就有以下几个操作：

1. 创建权重和偏置
2. 在输入上进行卷积
3. 把偏置加在卷积结果上
4. 用激活函数作用于以上结果

如果你使用普通的TF代码，写一个卷积层你需要这么多代码：
```python
input = ...
with tf.name_scope('conv1_1') as scope:
  kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,
                                           stddev=1e-1), name='weights')
  conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')
  biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),
                       trainable=True, name='biases')
  bias = tf.nn.bias_add(conv, biases)
  conv1 = tf.nn.relu(bias, name=scope)
```
为了简化以上操作（也就是我自己之前写的包装，当然还可以加入更多功能），用TF-Slime写只需要一行
```python
input = ...
net = slim.conv2d(input, 128, [3, 3], scope='conv1_1')
```
更可怕的，TF-Slim提供两个元操作`repeat`和`stack`来让你写出更加简洁的代码来重复进行相同的操作。比如VGG网络中，有这种重复的代码
```python
net = ...
net = slim.conv2d(net, 256, [3, 3], scope='conv3_1')
net = slim.conv2d(net, 256, [3, 3], scope='conv3_2')
net = slim.conv2d(net, 256, [3, 3], scope='conv3_3')
net = slim.max_pool2d(net, [2, 2], scope='pool2')
```
有一种方法是可以这么写，使用`for`循环
```python
net = ...
for i in range(3):
  net = slim.conv2d(net, 256, [3, 3], scope='conv3_' % (i+1))
net = slim.max_pool2d(net, [2, 2], scope='pool2')
```
而用TF-Slim的`repeat`操作符
```python
net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
net = slim.max_pool2d(net, [2, 2], scope='pool2')
```   
`slim.repeat`不仅重复使用相同的argument，而且它能够自己展开scope，比如上面的，就会被展开命名为'conv3/conv3_1', 'conv3/conv3_2', 'conv3/conv3_3'。

`slim.stack`操作符则允许调用者重复调用相同的操作而使用不同的argument来创建层。
```python
# Verbose way:
x = slim.fully_connected(x, 32, scope='fc/fc_1')
x = slim.fully_connected(x, 64, scope='fc/fc_2')
x = slim.fully_connected(x, 128, scope='fc/fc_3')

# Equivalent, TF-Slim way using slim.stack:
slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')
```
```python
# Verbose way:
x = slim.conv2d(x, 32, [3, 3], scope='core/core_1')
x = slim.conv2d(x, 32, [1, 1], scope='core/core_2')
x = slim.conv2d(x, 64, [3, 3], scope='core/core_3')
x = slim.conv2d(x, 64, [1, 1], scope='core/core_4')

# Using stack:
slim.stack(x, slim.conv2d, [(32, [3, 3]), (32, [1, 1]), (64, [3, 3]), (64, [1, 1])], scope='core')
```

### Scopes
处理TensorFlow本身的范围机制`name_scope`，`variable_scope`，TF-Slim还增加了`arg_scope`范围机制。这个新的范围允许使用者来明确表示一个或多个操作和一系列的arguments（会被传到对应的操作中）。还是用例子来解释吧：
```python
net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME',
                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv1')
net = slim.conv2d(net, 128, [11, 11], padding='VALID',
                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv2')
net = slim.conv2d(net, 256, [11, 11], padding='SAME',
                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv3')
```    
显然，上面三个卷积层使用很多相同的参数。比如两个使用相同的padding，三个使用相同的权重初始方法和权重调整方法。这样的代码十分冗余，也很难读，包含一堆重复的值。对于经常编程的人来说，有一个习惯可以解决以上问题，就是使用变量来表达默认值：
```python
padding = 'SAME'
initializer = tf.truncated_normal_initializer(stddev=0.01)
regularizer = slim.l2_regularizer(0.0005)
net = slim.conv2d(inputs, 64, [11, 11], 4,
                  padding=padding,
                  weights_initializer=initializer,
                  weights_regularizer=regularizer,
                  scope='conv1')
net = slim.conv2d(net, 128, [11, 11],
                  padding='VALID',
                  weights_initializer=initializer,
                  weights_regularizer=regularizer,
                  scope='conv2')
net = slim.conv2d(net, 256, [11, 11],
                  padding=padding,
                  weights_initializer=initializer,
                  weights_regularizer=regularizer,
                  scope='conv3')
```
但是有更加简便的方法，就是使用`arg_scope`。
```python
  with slim.arg_scope([slim.conv2d], padding='SAME',
                      weights_initializer=tf.truncated_normal_initializer(stddev=0.01)
                      weights_regularizer=slim.l2_regularizer(0.0005)):
    net = slim.conv2d(inputs, 64, [11, 11], scope='conv1')
    net = slim.conv2d(net, 128, [11, 11], padding='VALID', scope='conv2')
    net = slim.conv2d(net, 256, [11, 11], scope='conv3')

```
第二个从重写了Padding参数。还可以使用嵌入式`arg_scopes`
```python
with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
                      weights_regularizer=slim.l2_regularizer(0.0005)):
  with slim.arg_scope([slim.conv2d], stride=1, padding='SAME'):
    net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID', scope='conv1')
    net = slim.conv2d(net, 256, [5, 5],
                      weights_initializer=tf.truncated_normal_initializer(stddev=0.03),
                      scope='conv2')
    net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc')
```
举个栗子
写个VGGnet
```python
def vgg16(inputs):
  with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),
                      weights_regularizer=slim.l2_regularizer(0.0005)):
    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
    net = slim.max_pool2d(net, [2, 2], scope='pool1')
    net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
    net = slim.max_pool2d(net, [2, 2], scope='pool2')
    net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
    net = slim.max_pool2d(net, [2, 2], scope='pool3')
    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')
    net = slim.max_pool2d(net, [2, 2], scope='pool4')
    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
    net = slim.max_pool2d(net, [2, 2], scope='pool5')
    net = slim.fully_connected(net, 4096, scope='fc6')
    net = slim.dropout(net, 0.5, scope='dropout6')
    net = slim.fully_connected(net, 4096, scope='fc7')
    net = slim.dropout(net, 0.5, scope='dropout7')
    net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc8')
  return net
```
就是这么简单。

## Training Models
